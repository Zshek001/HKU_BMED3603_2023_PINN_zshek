{"cells":[{"cell_type":"markdown","metadata":{"id":"xOEO7uGv7X_n"},"source":["## Blood Flow Velocity Recovery with Physics-informed Neural Networks (PINN)\n","\\\\\n","Complete ##TODOs and make sure your code generate the correct result\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Phd95vAELaPT"},"outputs":[],"source":["### if you are using Google Colab\n","\n","# from google.colab import drive\n","# drive.mount('/content/drive')\n","# import os\n","# os.chdir('/content/drive/MyDrive/pinn')"]},{"cell_type":"markdown","metadata":{"id":"c3Uc8UwnKTh6"},"source":["### Import Libraries"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"69X_TDtTKpVS"},"outputs":[],"source":["import pandas as pd\n","import numpy as np\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import copy\n","import math\n","import os\n","import gc\n","import time\n","import random\n","# plotting\n","import matplotlib.pylab as plt\n","import matplotlib.ticker as ticker\n","# system\n","from time import time\n","import sys\n","# torch import\n","import torch\n","import torch.nn as nn\n","import torchvision.transforms as transforms\n","import torch.nn.functional as F\n","from torch.autograd import Variable\n","from torch.nn.parameter import Parameter\n","from torch.optim.lr_scheduler import ReduceLROnPlateau\n","import torch.optim as optim\n"]},{"cell_type":"markdown","metadata":{"id":"uHKsWe078oJw"},"source":["### Prepare Dataset"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"gOs7wPEmOuOR"},"outputs":[],"source":["### This function is to help you understand the meanings of variables in the dataset and load x y coordinates for training.\n","def dataloader():\n","\n","\t\ttrain_size = 64  ### tune train_size if you like\n","\t\tData = np.load('xyuvp.npz')  ### full data\n","\t\tData_sparse = np.load('xyuvp_sparse_separate_3sec.npz') ### data file contains sparse sampled data\n","\t\t### make sure you know what variables mean below. You will need to build inlet and outlet by yourself\n","\t\t### These names should be straightforward\n","\t\txinlet = Data_sparse['xinlet']\n","\t\tyinlet = Data_sparse['yinlet']\n","\t\tuinlet = Data_sparse['uinlet'] ### velocity at inlet\n","\n","\t\txoutlet = Data_sparse['xoutlet']\n","\t\tyoutlet = Data_sparse['youtlet']\n","\t\tuoutlet = Data_sparse['uoutlet'] ### velocity at outlet\n","\t\tx = Data['x']\n","\t\ty = Data['y']\n","\t\tu = Data['u']\n","\t\tv = Data['v']\n","\t\tP = Data['p']\n","\n","\t\tx = x[...,None]\n","\t\ty = y[...,None]\n","\t\tu = u[...,None]\n","\t\tv = v[...,None]\n","\t\tP = P[...,None]\n","\n","\t\txb = Data_sparse['xb']   ### boundary of x\n","\t\tyb = Data_sparse['yb']\n","\t\tub = Data_sparse['ub']\n","\t\tvb = Data_sparse['vb']\n","\t\tpb = Data_sparse['pb']\n","\n","\n","\n","\t\tsp_xdom = Data_sparse['xdom']   ### sparse sampled data\n","\t\tsp_ydom = Data_sparse['ydom']\n","\t\tsp_udom = Data_sparse['udom']   ### sparse sampled data\n","\t\tsp_vdom = Data_sparse['vdom']\n","\t\tsp_Pdom = Data_sparse['pdom']\n","\n","\n","\t\t\n","\t\tplt.figure(figsize=(12,8))\n","\t\tplt.scatter(x,y, c=u, cmap = 'coolwarm')\n","\t\tplt.colorbar()\n","\t\tplt.scatter(sp_xdom,sp_ydom, marker='x', c='black') ### plot sparse sampled data\n","\t\tplt.scatter(xb,yb, marker='x', c='orange') ### plot boundary data\n","\t\tplt.axis('equal')\n","\t\tplt.show()\n","\t\t############################\n","\n","\t\tnp.savez('stenosis_hard_coord',x = x,y = y,u = u, v = v, P = P)\n","\t\t################\n","\t\tdata = torch.utils.data.TensorDataset(torch.FloatTensor(x), torch.FloatTensor(y))\n","\t\t\n","\t\ttrain_loader = torch.utils.data.DataLoader(data, batch_size=train_size, shuffle=True)\n","\t\tprint('len(data is)', len(data))\n","\t\tprint('len(dataloader is)', len(train_loader))\n","\t\treturn train_loader,train_size\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"fRDWC5vUOuOR"},"outputs":[],"source":["train_loader, train_size = dataloader()"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"RjrXxS6mfS04"},"source":["### Build Neural Networks (30 Points)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ZWF7Tx9LOuOR"},"outputs":[],"source":["class Swish(nn.Module):\n","        def __init__(self, inplace=True):\n","            super(Swish, self).__init__()\n","            self.inplace = inplace\n","\n","        def forward(self, x):\n","            if self.inplace:\n","                x.mul_(torch.sigmoid(x))\n","                return x\n","            else:\n","                return x * torch.sigmoid(x)\n","class BloodNet(torch.nn.Module):\n","    def __init__(self, n_feature, n_hidden):\n","        super(BloodNet, self).__init__()\n","        self.nu = 1e-3 ### viscosity\n","        self.rho = 1 ### density\n","\n","\n","        ### TODO\n","        ''' Build a five-layer Feedforward Neural Networks\n","        Refer to: https://pytorch.org/docs/stable/generated/torch.nn.Linear.html\n","        https://pytorch.org/docs/stable/nn.html#non-linear-activations-weighted-sum-nonlinearity\n","        Often used Non-linearities: Swish(provided above), ReLU, Softmax, Sigmoid, Tanh...\n","        Inputs: 2D tensor x and y\n","        Outputs: 3D tensor u, v, P'''\n","\n","\n","        \n","        \n","        \n","    def forward(self, x):\n","      ### TODO\n","      '''Run x through the neural networks. Output u, v, P'''"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"IO-ycYxJOuOT"},"outputs":[],"source":["model = BloodNet(2,150)\n","device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n","model = model.to(device)\n","optimizer = optim.Adam(model.parameters(), lr=1e-3)\n","scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=1000, gamma=0.1)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"qd54bbIqI8Bo"},"outputs":[],"source":["### Optional Initialization\n","def init_normal(m):\n","\t\tif type(m) == nn.Linear:\n","\t\t\tnn.init.kaiming_normal_(m.weight)\n","model.apply(init_normal)"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"E7amPNKvJRg-"},"source":["### Define Loss Functions (20 Points)\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"bxf4rjlpOuOT"},"outputs":[],"source":["def criterion(model, x,y):\n","\n","\n","    x = torch.FloatTensor(x).to(device)\n","    y = torch.FloatTensor(y).to(device)\n","    x.requires_grad = True\n","    y.requires_grad = True\n","\n","\n","\n","\n","    net_in = torch.cat((x,y),1)\n","\n","    output = model.forward(net_in)\n","    u = output[:,0]\n","    v = output[:,1]\n","    P = output[:,2]\n","    u = u.view(len(u),-1)\n","    v = v.view(len(v),-1)\n","    P = P.view(len(P),-1)\n","\n","\n","\n","\n","\n","    ###Compute partial derivatives\n","    u_x = torch.autograd.grad(u,x,grad_outputs=torch.ones_like(x),create_graph = True,only_inputs=True)[0]   ### du/dx\n","    u_xx = torch.autograd.grad(u_x,x,grad_outputs=torch.ones_like(x),create_graph = True,only_inputs=True)[0]   \n","    u_y = torch.autograd.grad(u,y,grad_outputs=torch.ones_like(y),create_graph = True,only_inputs=True)[0]\n","    u_yy = torch.autograd.grad(u_y,y,grad_outputs=torch.ones_like(y),create_graph = True,only_inputs=True)[0]\n","    v_x = torch.autograd.grad(v,x,grad_outputs=torch.ones_like(x),create_graph = True,only_inputs=True)[0]\n","    v_xx = torch.autograd.grad(v_x,x,grad_outputs=torch.ones_like(x),create_graph = True,only_inputs=True)[0]\n","    v_y = torch.autograd.grad(v,y,grad_outputs=torch.ones_like(y),create_graph = True,only_inputs=True)[0]\n","    v_yy = torch.autograd.grad(v_y,y,grad_outputs=torch.ones_like(y),create_graph = True,only_inputs=True)[0]\n","\n","    P_x = torch.autograd.grad(P,x,grad_outputs=torch.ones_like(x),create_graph = True,only_inputs=True)[0]\n","    P_y = torch.autograd.grad(P,y,grad_outputs=torch.ones_like(y),create_graph = True,only_inputs=True)[0]\n","    ### TODO\n","    '''Implement the equation loss using partial derivatives computed above\n","    X-direction:\n","    Y-direction: \n","    Continuity:  '''\n","\n","\n","\n","    \n","    loss_f = nn.MSELoss()\n","\n","    #Note our target is zero. It is residual so we use zeros_like\n","    loss = loss_f(loss_1,torch.zeros_like(loss_1))+  loss_f(loss_2,torch.zeros_like(loss_2))+  loss_f(loss_3,torch.zeros_like(loss_3))\n","\n","\n","    return loss\n","def Loss_BC(model,xb,yb, inlet_input, outlet_input, inlet_target, outlet_target  ):\n","\n","    net_in1 = torch.cat((xb, yb), 1)\n","    output = model.forward(net_in1)\n","    u = output[:,0]\n","    v = output[:,1]  \n","    u = u.view(len(u),-1)\n","    v = v.view(len(v),-1)\n","    loss_f = nn.MSELoss()\n","    loss_noslip = loss_f(u, torch.zeros_like(u))\n","\n","\n","    inlet_out = model.forward(inlet_input)\n","    outlet_out = model.forward(outlet_input)\n","    loss_ic = loss_f(inlet_out[:,0], inlet_target[:,0]) + loss_f(outlet_out[:,0], outlet_target[:,0])\n","\n","\n","    return loss_noslip+loss_ic\n","\n","\n","\n","\n","def Loss_data(model,xd,yd,ud,vd):\n","\n","    net_in1 = torch.cat((xd, yd), 1)\n","    output = model.forward(net_in1)\n","    u = output[:,0]\n","    v = output[:,1]\n","    p = output[:,2]\n","    \n","    u = u.view(len(u),-1)\n","    v = v.view(len(v),-1)\n","    p = p.view(len(p),-1)\n","    \n","    loss_f = nn.MSELoss()\n","    loss_d = loss_f(u, ud) \n","\n","\n","    return loss_d\n","\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"pwYpPPPufpsh"},"source":["### Training Neural Networks (50 Points)\n","\n","Evaluation Criterion: \n","1. Plot of PINN prediction is similar to CFD simulation\n","2. loss_data is smaller than 1e-3"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"xZuX6A1MLib_"},"outputs":[],"source":["import time\n","\n","tic = time.time()\n","### Load Sparse Data and Boundary Data\n","Data_sparse = np.load('xyuvp_sparse_separate_3sec.npz')\n","\n","sp_xin = Data_sparse['xinlet']\n","sp_yin = Data_sparse['yinlet']\n","sp_uin = Data_sparse['uinlet']\n","sp_xout = Data_sparse['xoutlet']\n","sp_yout = Data_sparse['youtlet']\n","sp_uout = Data_sparse['uoutlet']\n","sp_xdom = Data_sparse['xdom']\n","sp_ydom = Data_sparse['ydom']\n","sp_udom = Data_sparse['udom']\n","sp_vdom = Data_sparse['vdom']\n","sp_Pdom = Data_sparse['pdom']\n","sp_xb = Data_sparse['xb']\n","sp_yb = Data_sparse['yb']\n","sp_ub = Data_sparse['ub']\n","sp_vb = Data_sparse['vb']\n","sp_Pb = Data_sparse['pb']\n","\n","\n","sp_x = np.concatenate((sp_xdom,sp_xb),0)\n","sp_y = np.concatenate((sp_ydom,sp_yb),0)\n","sp_u = np.concatenate((sp_udom,sp_ub),0)\n","sp_v = np.concatenate((sp_vdom,sp_vb),0)\n","sp_P = np.concatenate((sp_Pdom,sp_Pb),0)\n","sp_x, sp_y, sp_u, sp_v, sp_P = sp_x[...,None], sp_y[...,None], sp_u[...,None], sp_v[...,None], sp_P[...,None]\n","sp_data = np.concatenate((sp_x,sp_y,sp_u,sp_v,sp_P),1)\n","\n","# for sparase stenosis\n","sp_x, sp_y, sp_u, sp_v, sp_P = torch.Tensor(sp_data[:,0]).to(device), torch.Tensor(sp_data[:,1]).to(device), torch.Tensor(sp_data[:,2]).to(device), torch.Tensor(sp_data[:,3]).to(device), torch.Tensor(sp_data[:,4]).to(device)\n","sp_x, sp_y, sp_u, sp_v, sp_P = sp_x.view(len(sp_x), -1), sp_y.view(len(sp_y), -1), sp_u.view(len(sp_u), -1), sp_v.view(len(sp_v), -1), sp_P.view(len(sp_P),-1)\n","\n","sp_x.reuqires_grad = True\n","sp_y.requires_grad = True\n","\n","\n","\n","xb = torch.Tensor(sp_xb).to(device)\n","yb = torch.Tensor(sp_yb).to(device)\n","xb = xb.view(len(xb),-1)\n","yb = yb.view(len(yb),-1)\n","xb.requires_grad = True\n","yb.requires_grad = True\n","x_in = torch.Tensor(sp_xin).to(device)\n","y_in = torch.Tensor(sp_yin).to(device)\n","u_in = torch.Tensor(sp_uin).to(device)\n","x_in, y_in, u_in = x_in.view(len(x_in),-1), y_in.view(len(y_in),-1), u_in.view(len(u_in),-1)\n","x_in.requires_grad = True\n","y_in.requires_grad = True\n","inlet_input = torch.cat((x_in,y_in),1)\n","inlet_target = torch.cat((u_in,torch.zeros_like(u_in),torch.zeros_like(u_in)),1)\n","\n","\n","x_out = torch.Tensor(sp_xout).to(device)\n","y_out = torch.Tensor(sp_yout).to(device)\n","u_out = torch.Tensor(sp_uout).to(device)\n","x_out, y_out, u_out = x_out.view(len(x_out),-1), y_out.view(len(y_out),-1), u_out.view(len(u_out),-1)\n","\n","x_out.requires_grad = True\n","y_out.requires_grad = True\n","outlet_input = torch.cat((x_out, y_out), 1)\n","outlet_target = torch.cat((u_out, torch.zeros_like(u_out), torch.zeros_like(u_out)), 1)\n","tol = 0\n","err = np.inf\n","\n","'''Change epo to change how many epochs to run! You can start with 2000, but sometimes 1000 is enough if you are lucky.\n","   Monitor the training process and stop if you think the result looks similar with CFD simulation '''\n","epo = 2000\n","\n","\n","for epoch in range(epo):\n","\n","    loss_eqn_tot = 0.\n","    loss_bc_tot = 0.\n","    loss_data_tot = 0.\n","    loss_tot = 0\n","    n = 0\n","    \n","\n","    for batch_idx, (x,y) in enumerate(train_loader): \n","        ### TODO\n","        '''Train the neural networks using the five steps we learned in class\n","           1. Forward the data through the model;\n","           2. Compute the loss (measure of fitting capability); call the loss functions defined above\n","           3. Zero the grad-parameters (partial derivative of the loss wrt the parameters);\n","           4. Accumulate the grad-parameters;\n","           5. Step in the opposite direction of the grad-parameters. '''\n","        \n","        \n","        \n","       \n","        \n","        loss_eqn_tot += loss_eqn\n","        loss_bc_tot += loss_bc\n","        loss_data_tot  += loss_data\n","        loss_tot += loss\n","\n","    scheduler.step()\n","    if epoch % 100 ==0:\n","      \n","\t\t\t\t\tData = np.load('stenosis_hard_coord.npz')\n","\t\t\t\t\tx = Data['x']\n","\t\t\t\t\ty = Data['y']\n","\t\t\t\t\tu_CFD = Data['u']\n","\n","\t\t\t\t\t\n","\t\t\t\t\txt,yt = torch.Tensor(x), torch.Tensor(y)\n","\t\t\t\t\txt,yt = xt.view(len(xt),-1), yt.view(len(yt),-1)\n","\t\t\t\t\txt.requires_grad = True\n","\t\t\t\t\tyt.requires_grad = True\n","\t\t\t\t\txt, yt = xt.to(device), yt.to(device)\n","\t\t\t\t\tinputs = torch.cat((xt,yt),1)\n","\t\t\t\t\twith torch.no_grad():\n","\t\t\t\t\t\t\tout_all = model.forward(inputs)\n","\t\t\t\t\t\t\tu_hard = out_all[:,0]\n","\t\t\t\t\t\t\tv_hard = out_all[:,1]\n","\t\t\t\t\t\t\tP_hard = out_all[:,2]\n","\t\t\t\t\t\t\tu_hard = u_hard.view(len(u_hard),-1)\n","\t\t\t\t\t\t\tv_hard = v_hard.view(len(v_hard),-1)\n","\t\t\t\t\t\t\tP_hard = P_hard.view(len(P_hard),-1)\n","\t\t\t\t\t\t\tu_hard = u_hard.cpu().detach().numpy()\n","\t\t\t\t\t\t\tv_hard = v_hard.cpu().detach().numpy()\n","\t\t\t\t\t\t\tP_hard = P_hard.cpu().detach().numpy()\n","\t\t\t\t\t\n","\n","\t\t\t\t\tplot_x= 0.4*np.max(x)\n","\t\t\t\t\tplot_y = 0.95*np.max(y)\n","\t\t\t\t\tfontsize = 18\n","\t\t\t\t\t\n","\n","\t\t\t\t\tplt.figure()\n","\t\t\t\t\tplt.subplot(2,1,1)\n","\t\t\t\t\tplt.scatter(x, y, c= u_hard, label = 'uhard', cmap = 'coolwarm', vmin = min(u_CFD), vmax = max(u_CFD))\n","\t\t\t\t\tplt.text(plot_x, plot_y, r'epoch {}'.format(epoch), {'color': 'b', 'fontsize': fontsize})\n","\t\t\t\t\tplt.colorbar()\n","\n","\t\t\t\t\tplt.savefig('./results/{}.png'.format(epoch),bbox_inches = 'tight')\n","\n","\n","    print('*****Train Epoch: {} Total Loss {:.10f} Loss eqn {:.10f} Loss BC {:.10f} Loss data {:.10f} ****'.format(epoch, loss_tot, loss_eqn_tot, loss_bc_tot,loss_data_tot) )\n","torch.save(model.state_dict(), 'pinn_stenosis.pt')\n","f'finished in {(time.time()-tic)/60}'"]}],"metadata":{"colab":{"collapsed_sections":[],"name":"PINN.ipynb","provenance":[]},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.3"}},"nbformat":4,"nbformat_minor":0}
